{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self,ndim):\n",
    "        super(LinearModel,self).__init__()\n",
    "        \n",
    "        self.ndim = ndim \n",
    "        self.weight = nn.Parameter(torch.randn(ndim,1))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #y = Wx+b\n",
    "        return x.mm(self.weight)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearModel(13)#构建13个参数的线性回归模型实例\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#优化器（这里构建了一个随机梯度下降算法的优化器torch.optim.SGD）\n",
    "optim = torch.optim.SGD(lm.parameters(),lr=1e-6)\n",
    "#优化器的第一个参数为：线性回归模型参数的生成器（调用lm.parameters方法）;第二个参数为lr学习率\n",
    "\n",
    "#构建训练输入特征data和预测目标target\n",
    "data = torch.tensor(boston[\"data\"],requires_grad=True,dtype=torch.float32)#传入的boston[\"data\"]的numpy数组是双精度的，所以使用dtype转换为单精度的\n",
    "target = torch.tensor(boston[\"data\"],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'为了实现前向和反向传播计算，所以在构建输入特征data时，设置了requires_grad=True,，这样就能在计算过程中构建计算图'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''为了实现前向和反向传播计算，所以在构建输入特征data时，设置了requires_grad=True,，这样就能在计算过程中构建计算图'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([506, 13])) that is different to the input size (torch.Size([506, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:21780.156\n",
      "Loss:21609.004\n",
      "Loss:21482.234\n",
      "Loss:21387.656\n",
      "Loss:21316.549\n",
      "Loss:21262.592\n",
      "Loss:21221.221\n",
      "Loss:21189.117\n",
      "Loss:21163.879\n"
     ]
    }
   ],
   "source": [
    "#优化模型，即优化损失函数\n",
    "#对于每个epoch：1. 获取当前预测结果 2. 根据预测结果计算损失函数 3.清零所有参数前一次的反向传播梯度 4.当前方向传播梯度计算 5. 梯度优化\n",
    "\n",
    "for epoch in range(1000):\n",
    "    #先获取当前参数下模型的预测结果\n",
    "    predict = lm(data)#使用这个预测结果，计算损失函数\n",
    "    loss = criterion(predict,target)\n",
    "\n",
    "    if epoch and epoch %100 ==0:\n",
    "        print(\"Loss:{:.3f}\".format(loss.item()))\n",
    "    optim.zero_grad()#清零梯度【清零所有参数前一次反向传播的梯度】\n",
    "    loss.backward()#反向传播，损失函数调用反向传播【计算得到每个参数当前反向传播的梯度】\n",
    "    optim.step()#得到每个参数梯度后，使用这个step方法（对传入参数生成器（以及传入列表或迭代器））中的每个元素进行优化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''通过字典的列表，可以分别指定学习率，达到对不同的参数使用不同学习率的目的'''\n",
    "optimizer = torch.optim.SGD(lm.parameters(),lr=1e-2,momentum=0.9)#momentum-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([506, 13])) that is different to the input size (torch.Size([506, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21019.9863, grad_fn=<MseLossBackward0>)\n",
      "Loss:21019.986\n",
      "tensor(21017.5430, grad_fn=<MseLossBackward0>)\n",
      "Loss:21017.543\n",
      "tensor(21015.1836, grad_fn=<MseLossBackward0>)\n",
      "Loss:21015.184\n",
      "tensor(21012.8984, grad_fn=<MseLossBackward0>)\n",
      "Loss:21012.898\n",
      "tensor(21010.6836, grad_fn=<MseLossBackward0>)\n",
      "Loss:21010.684\n",
      "tensor(21008.5371, grad_fn=<MseLossBackward0>)\n",
      "Loss:21008.537\n",
      "tensor(21006.4531, grad_fn=<MseLossBackward0>)\n",
      "Loss:21006.453\n",
      "tensor(21004.4238, grad_fn=<MseLossBackward0>)\n",
      "Loss:21004.424\n",
      "tensor(21002.4492, grad_fn=<MseLossBackward0>)\n",
      "Loss:21002.449\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    predict = lm(data)\n",
    "    loss = criterion(predict,target)#目标函数\n",
    "\n",
    "    if epoch and epoch %100 ==0:\n",
    "        print(loss)\n",
    "        print(\"Loss:{:.3f}\".format(loss.item()))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()#损失函数反向传播\n",
    "    optim.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d29624fa02f72a2f2eb64b5fa4dfbc751609e2b6c88be691c0db207c64cc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
